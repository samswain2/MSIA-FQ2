{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iLykei Lecture Series\n",
    "# Text Analytics (Northwestern University, MLDS 414)\n",
    "# Notebook: Cleaning Data\n",
    "\n",
    "\n",
    "## Yuri Balasanov, &copy; iLykei 2021-2023\n",
    "\n",
    "##### Main text: Deep Learning for Natural Language Processing, © 2019 Jason Brownlee.   \n",
    "See also the [blog by Jason Brownlee](https://machinelearningmastery.com/about/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metamorphosis by Franz Kafka\n",
    "\n",
    "For this example use text of Metamorphosis by Franz Kafka downloaded from [Project Gutenberg](http://www.gutenberg.org/cache/epub/5200/pg5200.txt). The file is in ASCII format, it contains the header and footer parts with meta information. \n",
    "\n",
    "Read the complete text (mode `rt`). Print out the header: everything until the first words \"One morning\". Print out the footer: everything after \"her young body.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADER:\n",
      "\n",
      " The Project Gutenberg EBook of Metamorphosis, by Franz Kafka\n",
      "Translated by David Wyllie.\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.net\n",
      "\n",
      "** This is a COPYRIGHTED Project Gutenberg eBook, Details Below **\n",
      "**     Please follow the copyright guidelines in this file.     **\n",
      "\n",
      "\n",
      "Title: Metamorphosis\n",
      "\n",
      "Author: Franz Kafka\n",
      "\n",
      "Translator: David Wyllie\n",
      "\n",
      "Release Date: August 16, 2005 [EBook #5200]\n",
      "First posted: May 13, 2002\n",
      "Last updated: May 20, 2012\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright (C) 2002 David Wyllie.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Metamorphosis\n",
      "  Franz Kafka\n",
      "\n",
      "Translated by David Wyllie\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      " \n",
      "END OF HEADER\n",
      "\n",
      "\n",
      "FOOTER: \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "End of the Project Gutenberg EBook of Metamorphosis, by Franz Kafka\n",
      "Translated by David Wyllie.\n",
      "\n",
      "*** END OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\n",
      "\n",
      "***** This file should be named 5200.txt or 5200.zip *****\n",
      "This and all associated files of various formats will be found in:\n",
      "        http://www.gutenberg.net/5/2/0/5200/\n",
      "\n",
      "\n",
      "\n",
      "Updated editions will replace the previous one--the old editions\n",
      "will be renamed.\n",
      "\n",
      "Creating the works from public domain print editions means that no\n",
      "one owns a United States copyright in these works, so the Foundation\n",
      "(and you!) can copy and distribute it in the United States without\n",
      "permission and without paying copyright royalties.  Special rules,\n",
      "set forth in the General Terms of Use part of this license, apply to\n",
      "copying and distributing Project Gutenberg-tm electronic works to\n",
      "protect the PROJECT GUTENBERG-tm concept and trademark.  Project\n",
      "Gutenberg is a registered trademark, and may not be used if you\n",
      "charge for the eBooks, unless you receive specific permission.  If you\n",
      "do not charge anything for copies of this eBook, complying with the\n",
      "rules is very easy.  You may use this eBook for nearly any purpose\n",
      "such as creation of derivative works, reports, performances and\n",
      "research.  They may be modified and printed and given away--you may do\n",
      "practically ANYTHING with public domain eBooks.  Redistribution is\n",
      "subject to the trademark license, especially commercial\n",
      "redistribution.\n",
      "\n",
      "\n",
      "\n",
      "*** START: FULL LICENSE ***\n",
      "\n",
      "THE FULL PROJECT GUTENBERG LICENSE\n",
      "PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\n",
      "\n",
      "To protect the Project Gutenberg-tm mission of promoting the free\n",
      "distribution of electronic works, by using or distributing this work\n",
      "(or any other work associated in any way with the phrase \"Project\n",
      "Gutenberg\"), you agree to comply with all the terms of the Full Project\n",
      "Gutenberg-tm License (available with this file or online at\n",
      "http://gutenberg.net/license).\n",
      "\n",
      "\n",
      "Section 1.  General Terms of Use and Redistributing Project Gutenberg-tm\n",
      "electroni \n",
      "\n",
      "------------------\n",
      " ates.  Compliance requirements are not uniform and it takes a\n",
      "considerable effort, much paperwork and many fees to meet and keep up\n",
      "with these requirements.  We do not solicit donations in locations\n",
      "where we have not received written confirmation of compliance.  To\n",
      "SEND DONATIONS or determine the status of compliance for any\n",
      "particular state visit http://pglaf.org\n",
      "\n",
      "While we cannot and do not solicit contributions from states where we\n",
      "have not met the solicitation requirements, we know of no prohibition\n",
      "against accepting unsolicited donations from donors in such states who\n",
      "approach us with offers to donate.\n",
      "\n",
      "International donations are gratefully accepted, but we cannot make\n",
      "any statements concerning tax treatment of donations received from\n",
      "outside the United States.  U.S. laws alone swamp our small staff.\n",
      "\n",
      "Please check the Project Gutenberg Web pages for current donation\n",
      "methods and addresses.  Donations are accepted in a number of other\n",
      "ways including including checks, online payments and credit card\n",
      "donations.  To donate, please visit: http://pglaf.org/donate\n",
      "\n",
      "\n",
      "Section 5.  General Information About Project Gutenberg-tm electronic\n",
      "works.\n",
      "\n",
      "Professor Michael S. Hart is the originator of the Project Gutenberg-tm\n",
      "concept of a library of electronic works that could be freely shared\n",
      "with anyone.  For thirty years, he produced and distributed Project\n",
      "Gutenberg-tm eBooks with only a loose network of volunteer support.\n",
      "\n",
      "Project Gutenberg-tm eBooks are often created from several printed\n",
      "editions, all of which are confirmed as Public Domain in the U.S.\n",
      "unless a copyright notice is included.  Thus, we do not necessarily\n",
      "keep eBooks in compliance with any particular paper edition.\n",
      "\n",
      "Most people start at our Web site which has the main PG search facility:\n",
      "\n",
      "http://www.gutenberg.net\n",
      "\n",
      "This Web site includes information about Project Gutenberg-tm,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newsletter to hear about new eBooks. \n",
      " END OF FOOTER\n"
     ]
    }
   ],
   "source": [
    "# load complete text\n",
    "filename = 'metamorphosis.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "print(\"HEADER:\\n\\n\",text[:831],\"\\nEND OF HEADER\\n\")\n",
    "print(\"\\nFOOTER: \\n\",text[119994:122000],\"\\n\")\n",
    "print(\"------------------\\n\",text[137000:],\"\\n END OF FOOTER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the header and the footer from the text file and save it as \"metamorphosis_clean.txt\". Load the clean file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermin.  He lay on\\nhis armour-like back, and if he lifted his head a little he could\\nsee his brown belly, slightly domed and divided by arches into stiff\\nsections.  The bedding was hardly able to cover it and seemed ready\\nto slide off any moment.  His many legs, pitifully thin compared\\nwith the size of the rest of him, waved about helplessly as he\\nlooked.\\n\\n\"What\\'s happened to me?\" he'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing text manually\n",
    "\n",
    "Before any further cleaning and preparation is done it is a good time now to explore the text. Here are some exploration results in the book, they can be found by searching:\n",
    "\n",
    "\n",
    "- \"It’s plain text so there is no markup to parse\n",
    "- The translation of the original German uses UK English (e.g. travelling)\n",
    "- The lines are artificially wrapped with new lines at about 70 characters\n",
    "- There are no obvious typos or spelling mistakes\n",
    "- There’s punctuation like commas, apostrophes, quotes, question marks, and more\n",
    "- There’s hyphenated descriptions like armour-like\n",
    "- There’s a lot of use of the em dash (-) to continue sentences (maybe replace with commas?)\n",
    "- There are names (e.g. Mr. Samsa)\n",
    "- There does not appear to be numbers that require handling (e.g. 1999)\n",
    "- There are section markers (e.g. II and III )\n",
    "\"    \n",
    "The text is already pretty clean. It can be prepared manually by following simple steps.\n",
    "\n",
    "## Cleaning and tokenization\n",
    "\n",
    "A common first step of text preparation is converting the whole text into a list of words. This will allow using the simplest models of NLP.\n",
    "\n",
    "Load the text without header and footer. Split it into words by white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such characteristics as punctuation, end of sentence are preserved: \"wasn't\", \"thought.\", \"me?\".\n",
    "\n",
    "Alternatively, tockenization of the text can be done using regex model pr regular expression operations (re) which splits the document into words by selecting strings of alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# split based on words only\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time tokens are formed differently: \"armour-like\" is now two words \"armour\" and \"like\", contractions are not hadled well, though: \"What’s\" also divided into two words: \"What\" and \"s\". This may not be the most convenient way. So, go back to splitting by white space and remove punctuation.\n",
    "\n",
    "## Punctuation\n",
    "\n",
    "Standard list of punctuation characters in python is given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "!\"\\#\\$%\\&'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\\\\\]\\^_`\\{\\|\\}\\~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "print(re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regular expressions to select punctuation characters and remove them using the sub()\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'Whats', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasnt', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# split into words by white space\n",
    "words = text.split()\n",
    "# prepare regex for char filtering\n",
    "# match any any character that is present in the string.punctuation module\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in words]\n",
    "print(stripped[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case if text contains some non-printable characters, like \"\\n\", \"\\t\" they can be removed similarly.\n",
    "\n",
    "`re_print = re.compile( ' [^%s] ' % re.escape(string.printable))`      \n",
    "`result = [re_print.sub( '' , w) for w in words]`\n",
    "\n",
    "## Normalization\n",
    "\n",
    "Finally, convert all words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'he', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'his', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"what\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'it', \"wasn't\", 'a', 'dream.', 'his', 'room,', 'a', 'proper', 'human']\n"
     ]
    }
   ],
   "source": [
    "# convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing text using NLTK\n",
    "\n",
    "When text files are much bigger than Kafka's Metamorphosis loading them into memory and cleaning manually may be challenging. NLTK provides good tools for that. See [Workshop, Part1](https://ilykei.com/api/fileProxy/documents%2FMachineLearning_iLykei%2FNLPIntroduction%2FiLykei_ML_NLP_Intro_1.ipynb) and [Workshop, Part 2](https://ilykei.com/api/fileProxy/documents%2FMachineLearning_iLykei%2FNLPIntroduction%2FiLykei_ML_NLP_Intro_2.ipynb) for review of NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download() # run it if NLTK data have not been downloaded yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "A useful first step in preparation of data provided by NLTK is splitting into sentences. Sentences may then be split into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\414_msia\\06_hw\\02_part_Data_Cleaning.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/06_hw/02_part_Data_Cleaning.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m \u001b[39mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/06_hw/02_part_Data_Cleaning.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# split into sentences\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/06_hw/02_part_Data_Cleaning.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sentences \u001b[39m=\u001b[39m sent_tokenize(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/06_hw/02_part_Data_Cleaning.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(sentences[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "# split into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK function `word_tokenize()` splits tokens based on white space and punctuation. For example, commas and periods become separate tokens and contractions are split apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '``', 'What', \"'s\", 'happened', 'to']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation\n",
    "\n",
    "Uninformative tokens like punctuation, can now be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 'happened', 'to', 'me', 'he', 'thought', 'It', 'was', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room']\n"
     ]
    }
   ],
   "source": [
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tokens like \"armour-like\" are removed.\n",
    "\n",
    "## Stop words\n",
    "\n",
    "Next remove uninformative words called \"stop words\", such as \"a\", \"the\", \"is\".   \n",
    "Print the standard set of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are all in lower case. They also contain both contractions, like \"wouldn't\" and they stripped versions \"wouldn\". As a general pipeline finish preparation by lowering the case and removing punctuation from all tokens. Then remove stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards', 'viewer']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still strange words remaining after removing stop words, like \"nt\", for example. Searching and cleaning such words, but retaining specific words for the given area of application may be a long and tedious process.    \n",
    "\n",
    "## Stemming\n",
    "\n",
    "Finally, normalize the text by stemming it using Porter Stemming algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'armourlik', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slightli', 'dome', 'divid', 'arch', 'stiff', 'section', 'bed', 'hardli', 'abl', 'cover', 'seem', 'readi', 'slide', 'moment', 'mani', 'leg', 'piti', 'thin', 'compar', 'size', 'rest', 'wave', 'helplessli', 'look', 'happen', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'littl', 'small', 'lay', 'peac', 'four', 'familiar', 'wall', 'collect', 'textil', 'sampl', 'lay', 'spread', 'tabl', 'samsa', 'travel', 'salesman', 'hung', 'pictur', 'recent', 'cut', 'illustr', 'magazin', 'hous', 'nice', 'gild', 'frame', 'show', 'ladi', 'fit', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'rais', 'heavi', 'fur', 'muff', 'cover', 'whole', 'lower', 'arm', 'toward', 'viewer']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# split into words\n",
    "#tokens = word_tokenize(text)\n",
    "# stemming of words\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in words]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data with Keras\n",
    "\n",
    "Text cannot be fed directly into deep learning models. It needs to be encoded in numbers.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Function `text to word sequence()` splits the text by white space, removes punctuation and converts words into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-20 15:44:44.425431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-20 15:44:44.592410: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-20 15:44:44.595687: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-20 15:44:44.595701: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-20 15:44:45.115830: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-20 15:44:45.115883: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-08-20 15:44:45.115890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "Keras function `one hot()` makes tokenization and encoding tokens with integers in one step. However, the function does not do one-hot encoding. Instead it is a wrapper around the `hashing trick()`. The use of a hash function means that not all words will be mapped into integer values uniquely. Similar to `text to word sequence()` the function does tokenization, removal of punctuation and lower case normalization. In order to define the dimension of the hashing space the function requires size of the vocabulary as an argument besides the text. This size can be the number of words in the document or larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_to_word_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\414_msia\\06_hw\\02_part_Data_Cleaning.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/06_hw/02_part_Data_Cleaning.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# estimate the size of the vocabulary\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/06_hw/02_part_Data_Cleaning.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(text_to_word_sequence(text))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/06_hw/02_part_Data_Cleaning.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(words)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/06_hw/02_part_Data_Cleaning.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(vocab_size)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_to_word_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `one_hot()` with the vocabulary size equal to the number of unique words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 5, 4, 3, 1, 1, 6, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "# integer encode the document\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A limitation of integer and count base encodings is that they require maintaining a vocabulary of words and their mapping to integers. As an alternative a one-way hash\n",
    "function converts words to integers without keeping track of a vocabulary. This\n",
    "is faster and requires less memory.    \n",
    "Keras function `hashing trick()` tokenizes the text and then encodes it with integers, just like the one hot() function. It allows specifying as hash function any hash function built in function md5 or any user-defined function. Below is an example of integer encoding a document using the md5 hash function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import hashing_trick\n",
    "# integer encode the document\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Within Keras there is a more sophisticated API provided by the class `Tokenizer`. This class prepares larger NLP projects for deep learning modeling.    \n",
    "The fitted `Tokenizer` returns 4 attributes:\n",
    "\n",
    "- word_count: A dictionary mapping of words and their occurrence counts when the Tokenizer was fit\n",
    "- word_docs: A dictionary mapping of words and the number of documents that reach appears in\n",
    "- word_index: A dictionary of words and their uniquely assigned integers\n",
    "- document_count: A dictionary mapping and the number of documents they appear in calculated during the fit.   \n",
    "\n",
    "After fitting `Tokenizer` to training data, it can be used to encode documents in both\n",
    "train and test datasets.    \n",
    "The function `texts_to_matrix()` can be used to create one vector per document. The length of the vectors is the total size of the vocabulary.    \n",
    "This function is common for text encoding for standard bag-of-words models. It works in several modes.     \n",
    "The available modes include:    \n",
    "\n",
    "- binary: Whether or not each word is present in the document. This is the default\n",
    "- count: The count of each word in the document\n",
    "- tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document\n",
    "- freq: The frequency of each word as a ratio of words within each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts: \n",
      " OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "\n",
      "Document count: \n",
      " 5\n",
      "\n",
      "Word index: \n",
      " {'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "\n",
      "Word docs: \n",
      " defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'work': 2, 'good': 1, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n",
      "\n",
      "Encoded docs: count\n",
      " [[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(\"Word counts: \\n\",t.word_counts)\n",
    "print(\"\\nDocument count: \\n\",t.document_count)\n",
    "print(\"\\nWord index: \\n\",t.word_index)\n",
    "print(\"\\nWord docs: \\n\",t.word_docs)\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(\"\\nEncoded docs: count\\n\",encoded_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
