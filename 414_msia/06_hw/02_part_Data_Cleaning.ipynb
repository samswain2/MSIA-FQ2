{
 cells [
  {
   cell_type markdown,
   metadata {},
   source [
    # iLykei Lecture Seriesn,
    # Text Analytics (Northwestern University, MLDS 414)n,
    # Notebook Cleaning Datan,
    n,
    n,
    ## Yuri Balasanov, &copy; iLykei 2021-2023n,
    n,
    ##### Main text Deep Learning for Natural Language Processing, © 2019 Jason Brownlee.   n,
    See also the [blog by Jason Brownlee](httpsmachinelearningmastery.comabout)
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    # Metamorphosis by Franz Kafkan,
    n,
    For this example use text of Metamorphosis by Franz Kafka downloaded from [Project Gutenberg](httpwww.gutenberg.orgcacheepub5200pg5200.txt). The file is in ASCII format, it contains the header and footer parts with meta information. n,
    n,
    Read the complete text (mode `rt`). Print out the header everything until the first words One morning. Print out the footer everything after her young body.
   ]
  },
  {
   cell_type code,
   execution_count 1,
   metadata {
    scrolled false
   },
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      HEADERn,
      n,
       The Project Gutenberg EBook of Metamorphosis, by Franz Kafkan,
      Translated by David Wyllie.n,
      n,
      This eBook is for the use of anyone anywhere at no cost and withn,
      almost no restrictions whatsoever.  You may copy it, give it away orn,
      re-use it under the terms of the Project Gutenberg License includedn,
      with this eBook or online at www.gutenberg.netn,
      n,
       This is a COPYRIGHTED Project Gutenberg eBook, Details Below n,
           Please follow the copyright guidelines in this file.     n,
      n,
      n,
      Title Metamorphosisn,
      n,
      Author Franz Kafkan,
      n,
      Translator David Wyllien,
      n,
      Release Date August 16, 2005 [EBook #5200]n,
      First posted May 13, 2002n,
      Last updated May 20, 2012n,
      n,
      Language Englishn,
      n,
      n,
       START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS n,
      n,
      n,
      n,
      n,
      Copyright (C) 2002 David Wyllie.n,
      n,
      n,
      n,
      n,
      n,
        Metamorphosisn,
        Franz Kafkan,
      n,
      Translated by David Wyllien,
      n,
      n,
      n,
      In,
      n,
      n,
       n,
      END OF HEADERn,
      n,
      n,
      FOOTER n,
       n,
      n,
      n,
      n,
      n,
      n,
      n,
      End of the Project Gutenberg EBook of Metamorphosis, by Franz Kafkan,
      Translated by David Wyllie.n,
      n,
       END OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS n,
      n,
       This file should be named 5200.txt or 5200.zip n,
      This and all associated files of various formats will be found inn,
              httpwww.gutenberg.net5205200n,
      n,
      n,
      n,
      Updated editions will replace the previous one--the old editionsn,
      will be renamed.n,
      n,
      Creating the works from public domain print editions means that non,
      one owns a United States copyright in these works, so the Foundationn,
      (and you!) can copy and distribute it in the United States withoutn,
      permission and without paying copyright royalties.  Special rules,n,
      set forth in the General Terms of Use part of this license, apply ton,
      copying and distributing Project Gutenberg-tm electronic works ton,
      protect the PROJECT GUTENBERG-tm concept and trademark.  Projectn,
      Gutenberg is a registered trademark, and may not be used if youn,
      charge for the eBooks, unless you receive specific permission.  If youn,
      do not charge anything for copies of this eBook, complying with then,
      rules is very easy.  You may use this eBook for nearly any purposen,
      such as creation of derivative works, reports, performances andn,
      research.  They may be modified and printed and given away--you may don,
      practically ANYTHING with public domain eBooks.  Redistribution isn,
      subject to the trademark license, especially commercialn,
      redistribution.n,
      n,
      n,
      n,
       START FULL LICENSE n,
      n,
      THE FULL PROJECT GUTENBERG LICENSEn,
      PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORKn,
      n,
      To protect the Project Gutenberg-tm mission of promoting the freen,
      distribution of electronic works, by using or distributing this workn,
      (or any other work associated in any way with the phrase Projectn,
      Gutenberg), you agree to comply with all the terms of the Full Projectn,
      Gutenberg-tm License (available with this file or online atn,
      httpgutenberg.netlicense).n,
      n,
      n,
      Section 1.  General Terms of Use and Redistributing Project Gutenberg-tmn,
      electroni n,
      n,
      ------------------n,
       ates.  Compliance requirements are not uniform and it takes an,
      considerable effort, much paperwork and many fees to meet and keep upn,
      with these requirements.  We do not solicit donations in locationsn,
      where we have not received written confirmation of compliance.  Ton,
      SEND DONATIONS or determine the status of compliance for anyn,
      particular state visit httppglaf.orgn,
      n,
      While we cannot and do not solicit contributions from states where wen,
      have not met the solicitation requirements, we know of no prohibitionn,
      against accepting unsolicited donations from donors in such states whon,
      approach us with offers to donate.n,
      n,
      International donations are gratefully accepted, but we cannot maken,
      any statements concerning tax treatment of donations received fromn,
      outside the United States.  U.S. laws alone swamp our small staff.n,
      n,
      Please check the Project Gutenberg Web pages for current donationn,
      methods and addresses.  Donations are accepted in a number of othern,
      ways including including checks, online payments and credit cardn,
      donations.  To donate, please visit httppglaf.orgdonaten,
      n,
      n,
      Section 5.  General Information About Project Gutenberg-tm electronicn,
      works.n,
      n,
      Professor Michael S. Hart is the originator of the Project Gutenberg-tmn,
      concept of a library of electronic works that could be freely sharedn,
      with anyone.  For thirty years, he produced and distributed Projectn,
      Gutenberg-tm eBooks with only a loose network of volunteer support.n,
      n,
      Project Gutenberg-tm eBooks are often created from several printedn,
      editions, all of which are confirmed as Public Domain in the U.S.n,
      unless a copyright notice is included.  Thus, we do not necessarilyn,
      keep eBooks in compliance with any particular paper edition.n,
      n,
      Most people start at our Web site which has the main PG search facilityn,
      n,
      httpwww.gutenberg.netn,
      n,
      This Web site includes information about Project Gutenberg-tm,n,
      including how to make donations to the Project Gutenberg Literaryn,
      Archive Foundation, how to help produce our new eBooks, and how ton,
      subscribe to our email newsletter to hear about new eBooks. n,
       END OF FOOTERn
     ]
    }
   ],
   source [
    # load complete textn,
    filename = 'metamorphosis.txt'n,
    file = open(filename, 'rt')n,
    text = file.read()n,
    file.close()n,
    print(HEADERnn,text[831],nEND OF HEADERn)n,
    print(nFOOTER n,text[119994122000],n)n,
    print(------------------n,text[137000],n END OF FOOTER)
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    Remove the header and the footer from the text file and save it as metamorphosis_clean.txt. Load the clean file.
   ]
  },
  {
   cell_type code,
   execution_count 2,
   metadata {},
   outputs [
    {
     data {
      textplain [
       'One morning, when Gregor Samsa woke from troubled dreams, he foundnhimself transformed in his bed into a horrible vermin.  He lay onnhis armour-like back, and if he lifted his head a little he couldnsee his brown belly, slightly domed and divided by arches into stiffnsections.  The bedding was hardly able to cover it and seemed readynto slide off any moment.  His many legs, pitifully thin comparednwith the size of the rest of him, waved about helplessly as henlooked.nnWhat's happened to me he'
      ]
     },
     execution_count 2,
     metadata {},
     output_type execute_result
    }
   ],
   source [
    # load textn,
    filename = 'metamorphosis_clean.txt'n,
    file = open(filename, 'rt')n,
    text = file.read()n,
    file.close()n,
    text[500]
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    # Preparing text manuallyn,
    n,
    Before any further cleaning and preparation is done it is a good time now to explore the text. Here are some exploration results in the book, they can be found by searchingn,
    n,
    n,
    - It’s plain text so there is no markup to parsen,
    - The translation of the original German uses UK English (e.g. travelling)n,
    - The lines are artificially wrapped with new lines at about 70 charactersn,
    - There are no obvious typos or spelling mistakesn,
    - There’s punctuation like commas, apostrophes, quotes, question marks, and moren,
    - There’s hyphenated descriptions like armour-liken,
    - There’s a lot of use of the em dash (-) to continue sentences (maybe replace with commas)n,
    - There are names (e.g. Mr. Samsa)n,
    - There does not appear to be numbers that require handling (e.g. 1999)n,
    - There are section markers (e.g. II and III )n,
        n,
    The text is already pretty clean. It can be prepared manually by following simple steps.n,
    n,
    ## Cleaning and tokenizationn,
    n,
    A common first step of text preparation is converting the whole text into a list of words. This will allow using the simplest models of NLP.n,
    n,
    Load the text without header and footer. Split it into words by white space.
   ]
  },
  {
   cell_type code,
   execution_count 3,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      ['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', 'What's', 'happened', 'to', 'me', 'he', 'thought.', 'It', wasn't, 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']n
     ]
    }
   ],
   source [
    # split into words by white spacen,
    words = text.split()n,
    print(words[100])
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    Such characteristics as punctuation, end of sentence are preserved wasn't, thought., me.n,
    n,
    Alternatively, tockenization of the text can be done using regex model pr regular expression operations (re) which splits the document into words by selecting strings of alphanumeric characters.
   ]
  },
  {
   cell_type code,
   execution_count 4,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      ['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His', 'room']n
     ]
    }
   ],
   source [
    import ren,
    # split based on words onlyn,
    words = re.split(r'W+', text)n,
    print(words[100])
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    This time tokens are formed differently armour-like is now two words armour and like, contractions are not hadled well, though What’s also divided into two words What and s. This may not be the most convenient way. So, go back to splitting by white space and remove punctuation.n,
    n,
    ## Punctuationn,
    n,
    Standard list of punctuation characters in python is given by
   ]
  },
  {
   cell_type code,
   execution_count 5,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      !#$%&'()+,-.;=@[]^_`{}~n,
      !#$%&'()+,-.;=@[]^_`{}~n
     ]
    }
   ],
   source [
    import stringn,
    print(string.punctuation)n,
    print(re.escape(string.punctuation))
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    Use regular expressions to select punctuation characters and remove them using the sub()n,
    function.
   ]
  },
  {
   cell_type code,
   execution_count 6,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      ['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'Whats', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasnt', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human']n
     ]
    }
   ],
   source [
    import stringn,
    # split into words by white spacen,
    words = text.split()n,
    # prepare regex for char filteringn,
    # match any any character that is present in the string.punctuation modulen,
    re_punc = re.compile('[%s]' % re.escape(string.punctuation))n,
    # remove punctuation from each wordn,
    stripped = [re_punc.sub('', w) for w in words]n,
    print(stripped[100])
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    In case if text contains some non-printable characters, like n, t they can be removed similarly.n,
    n,
    `re_print = re.compile( ' [^%s] ' % re.escape(string.printable))`      n,
    `result = [re_print.sub( '' , w) for w in words]`n,
    n,
    ## Normalizationn,
    n,
    Finally, convert all words to lower case.
   ]
  },
  {
   cell_type code,
   execution_count 7,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      ['one', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'he', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'his', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', 'what's', 'happened', 'to', 'me', 'he', 'thought.', 'it', wasn't, 'a', 'dream.', 'his', 'room,', 'a', 'proper', 'human']n
     ]
    }
   ],
   source [
    # convert to lower casen,
    words = [word.lower() for word in words]n,
    print(words[100])
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    # Preparing text using NLTKn,
    n,
    When text files are much bigger than Kafka's Metamorphosis loading them into memory and cleaning manually may be challenging. NLTK provides good tools for that. See [Workshop, Part1](httpsilykei.comapifileProxydocuments%2FMachineLearning_iLykei%2FNLPIntroduction%2FiLykei_ML_NLP_Intro_1.ipynb) and [Workshop, Part 2](httpsilykei.comapifileProxydocuments%2FMachineLearning_iLykei%2FNLPIntroduction%2FiLykei_ML_NLP_Intro_2.ipynb) for review of NLTK.
   ]
  },
  {
   cell_type code,
   execution_count 8,
   metadata {},
   outputs [],
   source [
    import nltkn,
    #nltk.download() # run it if NLTK data have not been downloaded yet
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Tokenizationn,
    n,
    A useful first step in preparation of data provided by NLTK is splitting into sentences. Sentences may then be split into words.
   ]
  },
  {
   cell_type code,
   execution_count 9,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      One morning, when Gregor Samsa woke from troubled dreams, he foundn,
      himself transformed in his bed into a horrible vermin.n
     ]
    }
   ],
   source [
    from nltk import sent_tokenizen,
    # split into sentencesn,
    sentences = sent_tokenize(text)n,
    print(sentences[0])
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    NLTK function `word_tokenize()` splits tokens based on white space and punctuation. For example, commas and periods become separate tokens and contractions are split apart.
   ]
  },
  {
   cell_type code,
   execution_count 10,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      ['One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '``', 'What', 's, 'happened', 'to']n
     ]
    }
   ],
   source [
    from nltk.tokenize import word_tokenizen,
    # split into wordsn,
    tokens = word_tokenize(text)n,
    print(tokens[100])
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Punctuationn,
    n,
    Uninformative tokens like punctuation, can now be filtered out.
   ]
  },
  {
   cell_type code,
   execution_count 11,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      ['One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 'happened', 'to', 'me', 'he', 'thought', 'It', 'was', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room']n
     ]
    }
   ],
   source [
    # remove all tokens that are not alphabeticn,
    words = [word for word in tokens if word.isalpha()]n,
    print(words[100])
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    Note that tokens like armour-like are removed.n,
    n,
    ## Stop wordsn,
    n,
    Next remove uninformative words called stop words, such as a, the, is.   n,
    Print the standard set of stop words.
   ]
  },
  {
   cell_type code,
   execution_count 12,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', you're, you've, you'll, you'd, 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', she's, 'her', 'hers', 'herself', 'it', it's, 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', that'll, 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', don't, 'should', should've, 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', aren't, 'couldn', couldn't, 'didn', didn't, 'doesn', doesn't, 'hadn', hadn't, 'hasn', hasn't, 'haven', haven't, 'isn', isn't, 'ma', 'mightn', mightn't, 'mustn', mustn't, 'needn', needn't, 'shan', shan't, 'shouldn', shouldn't, 'wasn', wasn't, 'weren', weren't, 'won', won't, 'wouldn', wouldn't]n
     ]
    }
   ],
   source [
    from nltk.corpus import stopwordsn,
    stop_words = stopwords.words('english')n,
    print(stop_words)
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    Stop words are all in lower case. They also contain both contractions, like wouldn't and they stripped versions wouldn. As a general pipeline finish preparation by lowering the case and removing punctuation from all tokens. Then remove stop words. 
   ]
  },
  {
   cell_type code,
   execution_count 13,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      ['one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards', 'viewer']n
     ]
    }
   ],
   source [
    import stringn,
    # convert to lower casen,
    tokens = [w.lower() for w in tokens]n,
    # prepare regex for char filteringn,
    re_punc = re.compile('[%s]' % re.escape(string.punctuation))n,
    # remove punctuation from each wordn,
    stripped = [re_punc.sub('', w) for w in tokens]n,
    # remove remaining tokens that are not alphabeticn,
    words = [word for word in stripped if word.isalpha()]n,
    # filter out stop wordsn,
    stop_words = set(stopwords.words('english'))n,
    words = [w for w in words if not w in stop_words]n,
    print(words[100])
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    There are still strange words remaining after removing stop words, like nt, for example. Searching and cleaning such words, but retaining specific words for the given area of application may be a long and tedious process.    n,
    n,
    ## Stemmingn,
    n,
    Finally, normalize the text by stemming it using Porter Stemming algorithm.
   ]
  },
  {
   cell_type code,
   execution_count 14,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      ['one', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'armourlik', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slightli', 'dome', 'divid', 'arch', 'stiff', 'section', 'bed', 'hardli', 'abl', 'cover', 'seem', 'readi', 'slide', 'moment', 'mani', 'leg', 'piti', 'thin', 'compar', 'size', 'rest', 'wave', 'helplessli', 'look', 'happen', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'littl', 'small', 'lay', 'peac', 'four', 'familiar', 'wall', 'collect', 'textil', 'sampl', 'lay', 'spread', 'tabl', 'samsa', 'travel', 'salesman', 'hung', 'pictur', 'recent', 'cut', 'illustr', 'magazin', 'hous', 'nice', 'gild', 'frame', 'show', 'ladi', 'fit', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'rais', 'heavi', 'fur', 'muff', 'cover', 'whole', 'lower', 'arm', 'toward', 'viewer']n
     ]
    }
   ],
   source [
    from nltk.stem.porter import PorterStemmern,
    # split into wordsn,
    #tokens = word_tokenize(text)n,
    # stemming of wordsn,
    porter = PorterStemmer()n,
    stemmed = [porter.stem(word) for word in words]n,
    print(stemmed[100])
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    # Preparing Data with Kerasn,
    n,
    Text cannot be fed directly into deep learning models. It needs to be encoded in numbers.n,
    n,
    ## Tokenizationn,
    n,
    Function `text to word sequence()` splits the text by white space, removes punctuation and converts words into lower case.
   ]
  },
  {
   cell_type code,
   execution_count 15,
   metadata {},
   outputs [
    {
     name stderr,
     output_type stream,
     text [
      2023-08-20 154444.425431 I tensorflowcoreplatformcpu_feature_guard.cc193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations  AVX2 AVX_VNNI FMAn,
      To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.n,
      2023-08-20 154444.592410 I tensorflowcoreutilport.cc104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.n,
      2023-08-20 154444.595687 W tensorflowcompilerxlastream_executorplatformdefaultdso_loader.cc64] Could not load dynamic library 'libcudart.so.11.0'; dlerror libcudart.so.11.0 cannot open shared object file No such file or directoryn,
      2023-08-20 154444.595701 I tensorflowcompilerxlastream_executorcudacudart_stub.cc29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.n,
      2023-08-20 154445.115830 W tensorflowcompilerxlastream_executorplatformdefaultdso_loader.cc64] Could not load dynamic library 'libnvinfer.so.7'; dlerror libnvinfer.so.7 cannot open shared object file No such file or directoryn,
      2023-08-20 154445.115883 W tensorflowcompilerxlastream_executorplatformdefaultdso_loader.cc64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror libnvinfer_plugin.so.7 cannot open shared object file No such file or directoryn,
      2023-08-20 154445.115890 W tensorflowcompilertf2tensorrtutilspy_utils.cc38] TF-TRT Warning Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.n
     ]
    },
    {
     name stdout,
     output_type stream,
     text [
      ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']n
     ]
    }
   ],
   source [
    from tensorflow.keras.preprocessing.text import text_to_word_sequencen,
    # define the documentn,
    text = 'The quick brown fox jumped over the lazy dog.'n,
    # tokenize the documentn,
    result = text_to_word_sequence(text)n,
    print(result)
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Encodingn,
    n,
    Keras function `one hot()` makes tokenization and encoding tokens with integers in one step. However, the function does not do one-hot encoding. Instead it is a wrapper around the `hashing trick()`. The use of a hash function means that not all words will be mapped into integer values uniquely. Similar to `text to word sequence()` the function does tokenization, removal of punctuation and lower case normalization. In order to define the dimension of the hashing space the function requires size of the vocabulary as an argument besides the text. This size can be the number of words in the document or larger.
   ]
  },
  {
   cell_type code,
   execution_count 16,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      8n
     ]
    }
   ],
   source [
    # estimate the size of the vocabularyn,
    words = set(text_to_word_sequence(text))n,
    vocab_size = len(words)n,
    print(vocab_size)
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    Apply `one_hot()` with the vocabulary size equal to the number of unique words in the document.
   ]
  },
  {
   cell_type code,
   execution_count 17,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      [6, 5, 4, 3, 1, 1, 6, 3, 6]n
     ]
    }
   ],
   source [
    from tensorflow.keras.preprocessing.text import one_hotn,
    # integer encode the documentn,
    result = one_hot(text, round(vocab_size1.3))n,
    print(result)
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    A limitation of integer and count base encodings is that they require maintaining a vocabulary of words and their mapping to integers. As an alternative a one-way hashn,
    function converts words to integers without keeping track of a vocabulary. Thisn,
    is faster and requires less memory.    n,
    Keras function `hashing trick()` tokenizes the text and then encodes it with integers, just like the one hot() function. It allows specifying as hash function any hash function built in function md5 or any user-defined function. Below is an example of integer encoding a document using the md5 hash function.
   ]
  },
  {
   cell_type code,
   execution_count 18,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      [6, 4, 1, 2, 7, 5, 6, 2, 6]n
     ]
    }
   ],
   source [
    from tensorflow.keras.preprocessing.text import hashing_trickn,
    # integer encode the documentn,
    result = hashing_trick(text, round(vocab_size1.3), hash_function='md5')n,
    print(result)
   ]
  },
  {
   cell_type markdown,
   metadata {},
   source [
    ## Tokenizern,
    n,
    Within Keras there is a more sophisticated API provided by the class `Tokenizer`. This class prepares larger NLP projects for deep learning modeling.    n,
    The fitted `Tokenizer` returns 4 attributesn,
    n,
    - word_count A dictionary mapping of words and their occurrence counts when the Tokenizer was fitn,
    - word_docs A dictionary mapping of words and the number of documents that reach appears inn,
    - word_index A dictionary of words and their uniquely assigned integersn,
    - document_count A dictionary mapping and the number of documents they appear in calculated during the fit.   n,
    n,
    After fitting `Tokenizer` to training data, it can be used to encode documents in bothn,
    train and test datasets.    n,
    The function `texts_to_matrix()` can be used to create one vector per document. The length of the vectors is the total size of the vocabulary.    n,
    This function is common for text encoding for standard bag-of-words models. It works in several modes.     n,
    The available modes include    n,
    n,
    - binary Whether or not each word is present in the document. This is the defaultn,
    - count The count of each word in the documentn,
    - tfidf The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the documentn,
    - freq The frequency of each word as a ratio of words within each document
   ]
  },
  {
   cell_type code,
   execution_count 19,
   metadata {},
   outputs [
    {
     name stdout,
     output_type stream,
     text [
      Word counts n,
       OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])n,
      n,
      Document count n,
       5n,
      n,
      Word index n,
       {'work' 1, 'well' 2, 'done' 3, 'good' 4, 'great' 5, 'effort' 6, 'nice' 7, 'excellent' 8}n,
      n,
      Word docs n,
       defaultdict(class 'int', {'well' 1, 'done' 1, 'work' 2, 'good' 1, 'effort' 1, 'great' 1, 'nice' 1, 'excellent' 1})n,
      n,
      Encoded docs countn,
       [[0. 0. 1. 1. 0. 0. 0. 0. 0.]n,
       [0. 1. 0. 0. 1. 0. 0. 0. 0.]n,
       [0. 0. 0. 0. 0. 1. 1. 0. 0.]n,
       [0. 1. 0. 0. 0. 0. 0. 1. 0.]n,
       [0. 0. 0. 0. 0. 0. 0. 0. 1.]]n
     ]
    }
   ],
   source [
    from tensorflow.keras.preprocessing.text import Tokenizern,
    # define 5 documentsn,
    docs = ['Well done!',n,
            'Good work',n,
            'Great effort',n,
            'nice work',n,
            'Excellent!']n,
    # create the tokenizern,
    t = Tokenizer()n,
    # fit the tokenizer on the documentsn,
    t.fit_on_texts(docs)n,
    # summarize what was learnedn,
    print(Word counts n,t.word_counts)n,
    print(nDocument count n,t.document_count)n,
    print(nWord index n,t.word_index)n,
    print(nWord docs n,t.word_docs)n,
    # integer encode documentsn,
    encoded_docs = t.texts_to_matrix(docs, mode='count')n,
    print(nEncoded docs countn,encoded_docs)
   ]
  }
 ],
 metadata {
  kernelspec {
   display_name Python 3 (ipykernel),
   language python,
   name python3
  },
  language_info {
   codemirror_mode {
    name ipython,
    version 3
   },
   file_extension .py,
   mimetype textx-python,
   name python,
   nbconvert_exporter python,
   pygments_lexer ipython3,
   version 3.10.12
  }
 },
 nbformat 4,
 nbformat_minor 2
}