{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ccc753c",
   "metadata": {},
   "source": [
    "# iLykei Lecture Series   \n",
    "# Generative AI App Client   \n",
    "\n",
    "### Y. Balasanov, A. Kobyshev, M. Tselishchev, &copy; iLykei 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ec1c58-ea2e-41bd-8786-52ef23ec8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libs & compile proto-files\n",
    "!protoc --python_out=./ *.proto\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cdist\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document\n",
    "loader = PyPDFLoader(\"./Generative_AI_App_Doc.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split document into smaller texts\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=50,\n",
    "    length_function=len, \n",
    "    add_start_index=True\n",
    "    )\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI API Key\n",
    "with open(\"OPENAI_KEY.txt\",'r') as f:\n",
    "    openai_api_key = f.readline().strip()\n",
    "openai.api_key = openai_api_key\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Embeddings Model\n",
    "embeddings_model = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache Functions\n",
    "def save_cache_to_file(cache, filename='embedding_cache.json'):\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(cache, file)\n",
    "\n",
    "def load_cache_from_file(filename='embedding_cache.json'):\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def get_embeddings(texts, embeddings_model, cache):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        text_hash = hash(text)\n",
    "        if text_hash in cache:\n",
    "            # Directly use the cached embedding\n",
    "            embedding = cache[text_hash]\n",
    "        else:\n",
    "            # Compute embedding and add to cache\n",
    "            embedding = embeddings_model.embed_documents([text])[0]\n",
    "            # Since embedding is already a list, directly add it to the cache\n",
    "            cache[text_hash] = embedding\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing cache and get embeddings\n",
    "embedding_cache = load_cache_from_file()\n",
    "embeddings = get_embeddings([doc.page_content for doc in texts], embeddings_model, embedding_cache)\n",
    "# save_cache_to_file(embedding_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to list format if necessary\n",
    "embeddings_as_lists = [embedding.tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in embeddings]\n",
    "\n",
    "# Ensure 'text_contents' and 'my_OpenAI_key' are defined in the global scope\n",
    "text_contents = [doc.page_content for doc in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424cd612",
   "metadata": {},
   "source": [
    "Define the event handler responding to test questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def find_similar_documents(question_embedding, embeddings, k=3):\n",
    "    similarities = cdist([question_embedding], embeddings, 'cosine')[0]\n",
    "    most_similar_indices = similarities.argsort()[:k]\n",
    "    return most_similar_indices\n",
    "\n",
    "def create_qa_prompt(content):\n",
    "    instructions = f\"\"\"\n",
    "    Please generate a SUCCINCT answer based on the context you are given. \n",
    "    Please answer as if you are answering a test question. \n",
    "    Please answer the question as it is asked, if it requires technicalities, please use them.\n",
    "\n",
    "    Here is the content of the document: {content}\n",
    "    \"\"\"\n",
    "    return instructions\n",
    "\n",
    "client = OpenAI()\n",
    "def get_openai_qa_response(prompt):\n",
    "\n",
    "    # Sleep so api doesn't hang\n",
    "    time.sleep(1)\n",
    "\n",
    "    try:\n",
    "        # Get the response from OpenAI\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            # model=\"gpt-4-1106-preview\",\n",
    "            messages=[{\"role\": \"system\", \"content\": \"Do EXACTLY as the instructions in the prompt say.\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            \n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in getting response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f227f61-3820-4aa3-a02c-72622f85a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def question_handler(question_id, question):\n",
    "    global embeddings_model, embeddings_as_lists, text_contents\n",
    "\n",
    "    print(f'{question_id}) Q: {question}')\n",
    "\n",
    "    # Generate embedding for the enhanced question\n",
    "    question_embedding = embeddings_model.embed_documents([question])[0]\n",
    "\n",
    "    # Find indices of k most similar documents\n",
    "    similar_docs_indices = find_similar_documents(question_embedding, embeddings_as_lists, k=1)\n",
    "\n",
    "    # Retrieve the texts of the similar documents\n",
    "    context_texts = [text_contents[idx] for idx in similar_docs_indices]\n",
    "\n",
    "    # Combine context texts into a single string\n",
    "    combined_context = \"\\n\".join(context_texts)\n",
    "\n",
    "    # Create a QA prompt based on the combined context\n",
    "    qa_prompt = create_qa_prompt(combined_context)\n",
    "\n",
    "    # Get the Q&A response from OpenAI\n",
    "    qa_response = get_openai_qa_response(qa_prompt)\n",
    "\n",
    "    # Print and return the response\n",
    "    print(f'A: {qa_response}\\n')\n",
    "    return qa_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bfe559",
   "metadata": {},
   "source": [
    "Instead of the line `answer = \"I don't know\"` insert the code denerating the answer by your app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec94b331-2b43-4433-8d5a-c805f402fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to datastream.ilykei.com:30095\n",
      "Sending login message\n",
      "Logged in successfully as  samuelswain2023@u.northwestern.edu\n",
      "0) Q: What is the specific cellular pathway targeted by PancreXcel in combatting pancreatic cancer?\n",
      "[0.08825718 0.09524833 0.09886445 0.12005709 0.12662714 0.10782266\n",
      " 0.10922715 0.15196166 0.22523841 0.12412656 0.13257508 0.1640101\n",
      " 0.17311522 0.17965879 0.22264847 0.15054686 0.15366384 0.14284379\n",
      " 0.14638702 0.1721993  0.17312325 0.18662241 0.18476963 0.17832097\n",
      " 0.12003877 0.17644506 0.15382626 0.27926297 0.26621417 0.14933377\n",
      " 0.10905529 0.14747703 0.14996883 0.17984852 0.14755263]\n",
      "A: The fusion protein in PancreXcel is designed to target the TGF-beta receptor, which is overproduced in pancreatic cancer. This allows PancreXcel to combat the disease by inhibiting the tumor's growth and evading the immune system.\n",
      "\n",
      "1) Q: How does PancreXcel disrupt the TGF-beta signaling pathway in pancreatic cancer?\n",
      "[0.10215161 0.08469624 0.11176082 0.14822202 0.15778702 0.12558281\n",
      " 0.13400919 0.18450526 0.23019907 0.16312779 0.1705402  0.19898571\n",
      " 0.20942164 0.19237845 0.24334356 0.18136707 0.18430113 0.17440746\n",
      " 0.1741675  0.20309866 0.19321837 0.22023129 0.20220605 0.21265778\n",
      " 0.1615592  0.18782343 0.17481486 0.28384963 0.26472562 0.17920876\n",
      " 0.12893196 0.17799964 0.1922496  0.21315348 0.18461342]\n",
      "A: PancreXcel is a targeted therapeutic approach for treating pancreatic cancer. It works by binding to the receptors overexpressed on cancer cells, inhibiting the usual interaction of TGF-beta with its receptors. This interruption of the signaling pathway hinders cancer cell proliferation, promotes cell death, and enhances the immune response against the tumor. PancreXcel aims to reduce tumor growth and improve patient outcomes in pancreatic cancer treatment. It is designed to minimize adverse effects on healthy tissues while maximizing its impact on cancer cells.\n",
      "\n",
      "2) Q: What is the primary goal of PancreXcel in the treatment of pancreatic cancer?\n",
      "[0.11053439 0.11801958 0.11220334 0.15177232 0.14883494 0.13754568\n",
      " 0.13102861 0.16183676 0.22333181 0.12879237 0.14029733 0.1687294\n",
      " 0.18294568 0.18728516 0.2283825  0.16091506 0.15418285 0.13466818\n",
      " 0.13754334 0.16660742 0.17534131 0.19216097 0.18878413 0.18743545\n",
      " 0.13308056 0.18322773 0.16042428 0.29005701 0.27294503 0.15452116\n",
      " 0.12665999 0.16151172 0.15496339 0.18684795 0.14492986]\n",
      "A: The fusion protein in PancreXcel is designed to target the TGF-beta receptor, which is overproduced in pancreatic cancer. This allows PancreXcel to combat the disease by inhibiting the tumor's growth and evading the immune system.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from Generative_AI_App_connection import connect\n",
    "\n",
    "with open(\"my_credentials.txt\",'r') as f:\n",
    "    lines = f.readlines()\n",
    "login, password = map(str.strip, lines)\n",
    "\n",
    "# server options\n",
    "host = 'datastream.ilykei.com' # do not change\n",
    "port = 30095   # do not change\n",
    "stream_name = 'Generative_AI_App'   # do not change\n",
    "catch_handler_errors = False  # we recommend using TRUE during the test and FALSE during workshop\n",
    "\n",
    "# make connection with your handler\n",
    "result = connect(host, port, login, password, stream_name,\n",
    "                 question_handler, catch_handler_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dacec7-46fc-446e-a09f-f24205180974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problems': [], 'n_signals': 10, 'penalty': 28, 'score': 72}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check results\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
