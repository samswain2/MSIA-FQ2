{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb75af1f",
   "metadata": {},
   "source": [
    "# iLykei Lecture Series   \n",
    "# Text Analytics (MLDS 414)   \n",
    "# Assignment: Sentiment Analysis with Naive Bayes Bag-of-Words Model \n",
    "\n",
    "### Y.Balasanov, M. Tselishchev, &copy; iLykei 2023\n",
    "\n",
    "## Preparing the data    \n",
    "\n",
    "Data for this project are in the form of a corpus of documents.   \n",
    "Each document is a tweet regarding an airline service.    \n",
    "The goal is to identify (predict) the sentiment of the document: +1 for positive, 0 - for neutral and -1 - for negative.   \n",
    "The training set contains the sentiment column in which allocation of sentiments was done by humans.    \n",
    "Vocabulary for this project is created from the table of all words in the corpus of documents.    \n",
    "\n",
    "Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ec1c58-ea2e-41bd-8786-52ef23ec8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q matplotlib numpy pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16f9511",
   "metadata": {},
   "source": [
    "Install `protobuf` following the instructions [here](https://github.com/protocolbuffers/protobuf/blob/main/src/README.md). Then run the following line, it should not result in any error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ce617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!protoc --python_out=./ *.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad8f38d-7a0b-40a7-b458-187fd5a28d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ca571f",
   "metadata": {},
   "source": [
    "Download NLTK modules with stopwords, punctuation, and wordnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124e265b-a441-456e-9be0-b0735f2fdcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nuke2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nuke2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nuke2\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d36af",
   "metadata": {},
   "source": [
    "Add some specific stopwords for this corpus.   \n",
    "Add lemmatizer based on WordNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93eab4a3-cd23-443e-990d-a8814c831df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aircompanies_accounts = ['VirginAmerica', 'United', 'SouthwestAir', 'JetBlue', \n",
    "                         'Delta', 'USAirways', 'AmericanAir']\n",
    "other_stopwords = ['fly', 'flying', 'flight', 'flights', 'plane']\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.extend([w.lower() for w in aircompanies_accounts])\n",
    "eng_stopwords.extend(other_stopwords)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8e348",
   "metadata": {},
   "source": [
    "Create function preparing bag-of-words documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9b5096-650f-4795-a4b0-5f59b786f0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(tweet):\n",
    "    # Remove everything but letters:\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n",
    "    # Make lower-case:\n",
    "    tweet = tweet.lower()\n",
    "    # Tokenize tweet:\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    # Remove stop-words:\n",
    "    tokens = list(filter(lambda token: token not in eng_stopwords, tokens))\n",
    "    # Lemmatize all tokens:\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece89f5e",
   "metadata": {},
   "source": [
    "Load pretrained models.   \n",
    "Note that this document uses only one model: binary Naive Bayes.    \n",
    "Train and load multinomial model to improve results.    \n",
    "Experiment with model ensembling if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc99032-15d7-4f39-98a9-d57df9ebf948",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = joblib.load('models/vectorizer_bernoulli.joblib')\n",
    "nb_model = joblib.load('models/nb_bernoulli.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f16c3",
   "metadata": {},
   "source": [
    "## Prepare the process of responding to the tweets in real time   \n",
    "\n",
    "Define global variables for the process.   \n",
    "Initialized data frame for received tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c14d9429-6a5b-4681-9cb3-dc1135def42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counter = 0     # tweet event counter\n",
    "BUF_SIZE = 1000       # no need to change this buffer size\n",
    "\n",
    "# we create buffers in advance:\n",
    "tweets_df = pd.DataFrame(index=range(0, BUF_SIZE),\n",
    "                         columns=['time', 'tweet_id', 'text', \n",
    "                                  'prob_neg', 'prob_neutral', 'prob_positive'])\n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce89d4",
   "metadata": {},
   "source": [
    "Define the event handler.     \n",
    "Event hadler is a function that executes the logic of responses to the incoming messages with tweets.    \n",
    "This function is automatically called every time a new message is received from the server. The function has the following steps:   \n",
    "\n",
    "- Identify time stamp;   \n",
    "- Update the data frame with received tweets;    \n",
    "- Tokenize the tweet and make it a bag-of-of words;   \n",
    "- Predict probabilities of classes. This step uses the pre-fitted model uploaded in the memory. The best model must be selected, or an ensembling logic with several models must be defined here;   \n",
    "- Update the data frame with the predicted probabilities.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f227f61-3820-4aa3-a02c-72622f85a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_handler(tweet_id, text):\n",
    "    global tweets_df, tweet_counter\n",
    "    now = datetime.now()\n",
    "    # update tweets_df dataframe:\n",
    "    tweets_df.loc[tweet_counter] = [now, tweet_id, text, np.nan, np.nan, np.nan]\n",
    "    tweet_counter += 1\n",
    "    # process new tweet\n",
    "    print(tweet_id, text)\n",
    "    tokens =  my_tokenizer(text)\n",
    "    matrix_model = vectorizer_model.transform([tokens]).toarray()\n",
    "    model_proba = nb_model.predict_proba(matrix_model)\n",
    "    probs = list(model_proba[0])\n",
    "    print(f'{probs=}')\n",
    "    tweets_df.loc[tweet_counter - 1, ['prob_neg', 'prob_neutral', 'prob_positive']] = probs\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23940e69",
   "metadata": {},
   "source": [
    "## Run the reali-time process   \n",
    "\n",
    "Connect to the server using your credentials stored in `my_credentials.txt`. The file must contain 2 lines: login name (email address) and the streaming password.    \n",
    "Connect and see how the handler with your model classifies the documents.   \n",
    "The score reflecting the accuracy of the classification of the test sample will appear in the log at the end of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec94b331-2b43-4433-8d5a-c805f402fc4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\414_msia\\03_hw\\iLykei_AirTweet_client.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/03_hw/iLykei_AirTweet_client.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mAirTweet_connection\u001b[39;00m \u001b[39mimport\u001b[39;00m connect\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/03_hw/iLykei_AirTweet_client.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmy_credentials.txt\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/03_hw/iLykei_AirTweet_client.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\414_msia\\03_hw\\AirTweet_connection.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# AirTweet_connection.py\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mAirTweet_pb2\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mAuthentication_pb2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msocket\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\414_msia\\03_hw\\AirTweet_pb2.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# source: AirTweet.proto\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minternal\u001b[39;00m \u001b[39mimport\u001b[39;00m builder \u001b[39mas\u001b[39;00m _builder\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m descriptor \u001b[39mas\u001b[39;00m _descriptor\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m descriptor_pool \u001b[39mas\u001b[39;00m _descriptor_pool\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (c:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\google\\protobuf\\internal\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from AirTweet_connection import connect\n",
    "\n",
    "with open(\"my_credentials.txt\",'r') as f:\n",
    "    lines = f.readlines()\n",
    "login, password = map(str.strip, lines)\n",
    "\n",
    "# server options; do not change\n",
    "host = 'datastream.ilykei.com'      \n",
    "port = 30019\n",
    "stream_name = 'AirTweet'\n",
    "catch_handler_errors = True  # we recommend using TRUE during the test and FALSE during preparation\n",
    "\n",
    "# make connection with your personal handlers\n",
    "result = connect(host, port, login, password, stream_name,\n",
    "                 tweet_handler, catch_handler_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a866fc",
   "metadata": {},
   "source": [
    "Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2dacec7-46fc-446e-a09f-f24205180974",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\414_msia\\03_hw\\iLykei_AirTweet_client.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/03_hw/iLykei_AirTweet_client.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e66f7f9-87c0-4cda-b3ad-b055159a3ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>prob_neg</th>\n",
       "      <th>prob_neutral</th>\n",
       "      <th>prob_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [time, tweet_id, text, prob_neg, prob_neutral, prob_positive]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove empty values from buffers\n",
    "tweets_df = tweets_df.head(tweet_counter)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cfb25f",
   "metadata": {},
   "source": [
    "Save the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f42d216f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\414_msia\\03_hw\\iLykei_AirTweet_client.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/03_hw/iLykei_AirTweet_client.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# after all you can dump your data/results and analyze it later\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/03_hw/iLykei_AirTweet_client.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mresults.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m output_f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/414_msia/03_hw/iLykei_AirTweet_client.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     pickle\u001b[39m.\u001b[39mdump([tweets_df, result], output_f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# after all you can dump your data/results and analyze it later\n",
    "with open('results.pkl', 'wb') as output_f:\n",
    "    pickle.dump([tweets_df, result], output_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58decf33",
   "metadata": {},
   "source": [
    "## Penalty Function    \n",
    "\n",
    "The penalty for this project is the logloss measure of accuracy of sentiment classification\n",
    "$$LogLoss=-\\frac{1}{N} \\sum_{i=1}^N \\left( y_{i,neg} \\log(p_{i,neg}) + y_{i,neut} \\log(p_{i,neut}) + y_{i,pos} \\log(p_{i,pos}) \\right),$$\n",
    "where $y_{i,c}=1$ when the tweet belongs to class $c$, and 0 otherwise; $p_{i,c}$ are predicted probabilities of classes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
