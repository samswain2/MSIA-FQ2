{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 17:42:39.620823: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-18 17:42:41.964994: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-18 17:42:48.732361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Suppress DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# List all GPUs available to TensorFlow\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Check if any GPUs are available\n",
    "if len(gpus) > 0:\n",
    "    print(\"GPU is available\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU device: {gpu}\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: Cartpole-v0\n",
    "env = gym.make(\n",
    "    \"CartPole-v0\",\n",
    "    # render_mode=\"human\"\n",
    "    )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "\n",
    "def CartPole_RL(model, discount_factor=0.95, num_choices=2):\n",
    "    \n",
    "    # Save rewards for visualization later\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(1000):\n",
    "        # print(f\"CartPole-v0, episode {episode}\")\n",
    "        # Initiate one episode\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        obs_history = []\n",
    "        reward_history = []\n",
    "        action_history = []\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "\n",
    "\n",
    "        # Roll out one episode\n",
    "        while (not terminated) and (not truncated):\n",
    "\n",
    "            # Make a prediction based on the state\n",
    "            probabilities = model.predict(np.array([observation]), verbose=0)[0]\n",
    "            # print(probabilities)\n",
    "\n",
    "            # This line stochastically samples an action based on the probabilities\n",
    "            action = np.random.choice(num_choices, p=probabilities)\n",
    "            # print(action)\n",
    "\n",
    "            obs_history.append(observation)\n",
    "            action_history.append(action)\n",
    "\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            print(observation, reward, terminated, truncated, info)\n",
    "\n",
    "            reward_history.append(reward)\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        # Discount rewards\n",
    "        discounted_rewards = []\n",
    "        cumulative_reward = 0\n",
    "        for reward in reversed(reward_history):\n",
    "            cumulative_reward = reward + discount_factor * cumulative_reward\n",
    "            discounted_rewards.insert(0, cumulative_reward)\n",
    "\n",
    "        # print(f\"Cumulative Reward History: {discounted_rewards}\")\n",
    "\n",
    "\n",
    "        # print(f\"Frame: {count_frame}\")\n",
    "        # print(f\"Obs History: {obs_history}\")\n",
    "        # print(f\"Reward History: {reward_history}\")\n",
    "        # print(f\"Action History: {action_history}\")\n",
    "\n",
    "\n",
    "\n",
    "        def adjust_weights(obs_history, action_history, discounted_rewards):\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Predict action probabilities\n",
    "                action_probabilities = model(np.array(obs_history))\n",
    "                # print(action_probabilities)\n",
    "\n",
    "                # Get probabilities of actions that were taken\n",
    "                indices = list(zip(range(len(action_history)), action_history))\n",
    "                # print(indices)\n",
    "                chosen_action_probs = tf.gather_nd(action_probabilities, indices)\n",
    "                # print(chosen_action_probs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = -tf.math.log(chosen_action_probs) * discounted_rewards\n",
    "                loss = tf.reduce_mean(loss)\n",
    "\n",
    "                \n",
    "            # Calculate gradients\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            # print(grads)\n",
    "            \n",
    "            # Apply gradients to model weights\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # # Print updated model weights\n",
    "            # for var in model.trainable_variables:\n",
    "            #     print(var.name, var.numpy()[1])\n",
    "\n",
    "\n",
    "        adjust_weights(obs_history=obs_history, action_history=action_history, discounted_rewards=discounted_rewards)\n",
    "\n",
    "\n",
    "        # Append episode rewards for plotting\n",
    "        total_reward = sum(reward_history)\n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"CartPole-v0 episode {episode}, reward sum: {total_reward}\")\n",
    "\n",
    "        # clear_output(wait=True)\n",
    "        \n",
    "    env.close()\n",
    "\n",
    "    # After the for loop\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\", input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "CartPole_RL(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (Pong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "def check_gpu_availability():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(\"GPU is available\")\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU device: {gpu}\")\n",
    "    else:\n",
    "        print(\"GPU is not available\")\n",
    "\n",
    "check_gpu_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU')\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "We're Unable to find the game \"Pong\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"Pong\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"Pong\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/nfs/home/sms5736/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmsia-deepdish4/nfs/home/sms5736/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=125'>126</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bmsia-deepdish4/nfs/home/sms5736/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=126'>127</a>\u001b[0m     check_gpu_availability()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bmsia-deepdish4/nfs/home/sms5736/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=127'>128</a>\u001b[0m     Pong_RL()\n",
      "\u001b[1;32m/nfs/home/sms5736/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmsia-deepdish4/nfs/home/sms5736/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mPong_RL\u001b[39m():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmsia-deepdish4/nfs/home/sms5736/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mPong-v0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmsia-deepdish4/nfs/home/sms5736/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmsia-deepdish4/nfs/home/sms5736/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     model \u001b[39m=\u001b[39m build_model()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gym/envs/registration.py:640\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     render_mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 640\u001b[0m     env \u001b[39m=\u001b[39m env_creator(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs)\n\u001b[1;32m    641\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    642\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    643\u001b[0m         \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mgot an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    644\u001b[0m         \u001b[39mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    645\u001b[0m     ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ale_py/env/gym.py:155\u001b[0m, in \u001b[0;36mAtariEnv.__init__\u001b[0;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space, max_num_frames_per_episode, render_mode)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39male\u001b[39m.\u001b[39msetBool(\u001b[39m\"\u001b[39m\u001b[39msound\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    154\u001b[0m \u001b[39m# Seed + Load\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseed()\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_set \u001b[39m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39male\u001b[39m.\u001b[39mgetLegalActionSet()\n\u001b[1;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m full_action_space\n\u001b[1;32m    160\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39male\u001b[39m.\u001b[39mgetMinimalActionSet()\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_space \u001b[39m=\u001b[39m spaces\u001b[39m.\u001b[39mDiscrete(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action_set))\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/ale_py/env/gym.py:206\u001b[0m, in \u001b[0;36mAtariEnv.seed\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39male\u001b[39m.\u001b[39msetInt(\u001b[39m\"\u001b[39m\u001b[39mrandom_seed\u001b[39m\u001b[39m\"\u001b[39m, seed2\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint32))\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(roms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_game):\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\n\u001b[1;32m    207\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWe\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mre Unable to find the game \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_game\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m. Note: Gym no longer distributes ROMs. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    208\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIf you own a license to use the necessary ROMs for research purposes you can download them \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvia `pip install gym[accept-rom-license]`. Otherwise, you should try importing \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_game\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    210\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvia the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_game\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    211\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis unsupported. To check if this is the case try providing the environment variable \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    212\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39male\u001b[39m.\u001b[39mloadROM(\u001b[39mgetattr\u001b[39m(roms, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_game))\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_game_mode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mError\u001b[0m: We're Unable to find the game \"Pong\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"Pong\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"Pong\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management"
     ]
    }
   ],
   "source": [
    "def preprocess(image, downsample_factor=2):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::downsample_factor,::downsample_factor,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    # In preprocess function\n",
    "    return np.reshape(image.astype(np.float64).ravel(), [80, 80])\n",
    "\n",
    "\n",
    "def build_model(input_shape=(80, 80, 1), num_choices=2, reg=0.0001):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(reg), input_shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(reg)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(num_choices, activation=\"softmax\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def select_action(model, observation):\n",
    "    probabilities = model.predict(np.array([observation]), verbose=0)[0]\n",
    "    action = np.random.choice([2, 3], p=probabilities)  # 2 is RIGHT and 3 is LEFT\n",
    "    return action\n",
    "\n",
    "def compute_discounted_rewards(reward_history, discount_factor=0.99):\n",
    "    discounted_rewards, cumulative_reward = [], 0\n",
    "    for reward in reversed(reward_history):\n",
    "        cumulative_reward = reward + discount_factor * cumulative_reward\n",
    "        discounted_rewards.insert(0, cumulative_reward)\n",
    "    return discounted_rewards\n",
    "\n",
    "def adjust_weights(model, optimizer, obs_history, action_history, discounted_rewards):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs=model(np.array(obs_history))\n",
    "        indices=tf.stack([tf.range(len(action_history),dtype=tf.int32),tf.convert_to_tensor(action_history,dtype=tf.int32)],axis=1)\n",
    "        chosen_probs=tf.gather_nd(probs,indices)\n",
    "        loss=-tf.math.log(chosen_probs)*discounted_rewards\n",
    "        loss=tf.reduce_mean(loss)\n",
    "    grads=tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "def Pong_RL():\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model = build_model()\n",
    "    episode_rewards = []\n",
    "    episode = 0\n",
    "\n",
    "    consecutive_21_rewards = 0  # Count number of 21 occurences\n",
    "    should_train = True  # Initialize flag for training\n",
    "\n",
    "    while True:\n",
    "        observation, info = env.reset()\n",
    "        obs_history, reward_history, action_history = [], [], []\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            processed_observation = preprocess(observation)\n",
    "            action = select_action(model, processed_observation)\n",
    "            obs_history.append(processed_observation)\n",
    "            action_history.append(action)\n",
    "\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            reward_history.append(reward)\n",
    "\n",
    "        discounted_rewards = compute_discounted_rewards(reward_history)\n",
    "\n",
    "        total_reward = sum(reward_history)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        moving_num, window = 0, 100\n",
    "        if episode >= window-1:\n",
    "            moving_avg = np.mean(episode_rewards[-window:])\n",
    "            print(f\"Pong-v0 episode {episode}, reward sum: {total_reward}, last {window} avg: {moving_avg:.2f}\")\n",
    "            \n",
    "            if moving_avg > moving_num:\n",
    "                print(f\"Stopping as the last {window}-episode moving average is greater than {moving_num}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Pong-v0 episode {episode}, reward sum: {total_reward}\")\n",
    "\n",
    "        ### TRAINING STOP\n",
    "\n",
    "        if total_reward == 21:  # Check for consecutive rewards of 21\n",
    "            consecutive_21_rewards += 1\n",
    "            if consecutive_21_rewards >= 10 and should_train == True:\n",
    "                print(\"Stopping training as the reward has been 21 for 10 episodes in a row\")\n",
    "                should_train = False  # Set flag to False\n",
    "\n",
    "                # Check if folder exists, if not create it\n",
    "                if not os.path.exists(\"saved_model\"):\n",
    "                    os.makedirs(\"saved_model\")\n",
    "\n",
    "                # Save the model\n",
    "                model.save(\"saved_model/pong_model\")\n",
    "                \n",
    "        else:\n",
    "            consecutive_21_rewards = 0  # Reset the counter if the reward is not 21\n",
    "\n",
    "        # Modify the weight adjustment to respect the should_train flag\n",
    "        if should_train:\n",
    "            adjust_weights(model, optimizer, obs_history, action_history, discounted_rewards)\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Save Results\n",
    "    if not os.path.exists('artifacts'):\n",
    "        os.makedirs('artifacts')\n",
    "\n",
    "    # Calculate moving average of rewards\n",
    "    window = 100  # Size of the window for calculating moving average\n",
    "    moving_avgs = [np.mean(episode_rewards[max(0, i - window + 1):i+1]) for i in range(len(episode_rewards))]\n",
    "\n",
    "    plt.plot(episode_rewards, label='Total Reward')\n",
    "    plt.plot(moving_avgs, label=f'{window}-Episode Moving Average')\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.legend()\n",
    "    plt.savefig('artifacts/pong_rewards.png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gpu_availability()\n",
    "    Pong_RL()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2916 into shape (80,80)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\490_msia\\01_hw\\01_hw_code.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Pong_RL()\n",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\490_msia\\01_hw\\01_hw_code.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X10sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m truncated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X10sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminated \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m truncated:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X10sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     processed_observation \u001b[39m=\u001b[39m preprocess(observation)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     action \u001b[39m=\u001b[39m select_action(model, processed_observation)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X10sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     obs_history\u001b[39m.\u001b[39mappend(processed_observation)\n",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\490_msia\\01_hw\\01_hw_code.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m image[image \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m# everything else (paddles, ball) just set to 1\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# In preprocess function\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mreshape(image\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat64)\u001b[39m.\u001b[39;49mravel(), [\u001b[39m80\u001b[39;49m, \u001b[39m80\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:285\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    202\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2916 into shape (80,80)"
     ]
    }
   ],
   "source": [
    "Pong_RL()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
