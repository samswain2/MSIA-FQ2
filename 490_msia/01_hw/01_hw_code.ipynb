{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Suppress DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# List all GPUs available to TensorFlow\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "# Check if any GPUs are available\n",
    "if len(gpus) > 0:\n",
    "    print(\"GPU is available\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"GPU device: {gpu}\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: Cartpole-v0\n",
    "env = gym.make(\n",
    "    \"CartPole-v0\",\n",
    "    # render_mode=\"human\"\n",
    "    )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "\n",
    "def CartPole_RL(model, discount_factor=0.95, num_choices=2):\n",
    "    \n",
    "    # Save rewards for visualization later\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(1000):\n",
    "        # print(f\"CartPole-v0, episode {episode}\")\n",
    "        # Initiate one episode\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        obs_history = []\n",
    "        reward_history = []\n",
    "        action_history = []\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "\n",
    "\n",
    "        # Roll out one episode\n",
    "        while (not terminated) and (not truncated):\n",
    "\n",
    "            # Make a prediction based on the state\n",
    "            probabilities = model.predict(np.array([observation]), verbose=0)[0]\n",
    "            # print(probabilities)\n",
    "\n",
    "            # This line stochastically samples an action based on the probabilities\n",
    "            action = np.random.choice(num_choices, p=probabilities)\n",
    "            # print(action)\n",
    "\n",
    "            obs_history.append(observation)\n",
    "            action_history.append(action)\n",
    "\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            print(observation, reward, terminated, truncated, info)\n",
    "\n",
    "            reward_history.append(reward)\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        # Discount rewards\n",
    "        discounted_rewards = []\n",
    "        cumulative_reward = 0\n",
    "        for reward in reversed(reward_history):\n",
    "            cumulative_reward = reward + discount_factor * cumulative_reward\n",
    "            discounted_rewards.insert(0, cumulative_reward)\n",
    "\n",
    "        # print(f\"Cumulative Reward History: {discounted_rewards}\")\n",
    "\n",
    "\n",
    "        # print(f\"Frame: {count_frame}\")\n",
    "        # print(f\"Obs History: {obs_history}\")\n",
    "        # print(f\"Reward History: {reward_history}\")\n",
    "        # print(f\"Action History: {action_history}\")\n",
    "\n",
    "\n",
    "\n",
    "        def adjust_weights(obs_history, action_history, discounted_rewards):\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Predict action probabilities\n",
    "                action_probabilities = model(np.array(obs_history))\n",
    "                # print(action_probabilities)\n",
    "\n",
    "                # Get probabilities of actions that were taken\n",
    "                indices = list(zip(range(len(action_history)), action_history))\n",
    "                # print(indices)\n",
    "                chosen_action_probs = tf.gather_nd(action_probabilities, indices)\n",
    "                # print(chosen_action_probs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = -tf.math.log(chosen_action_probs) * discounted_rewards\n",
    "                loss = tf.reduce_mean(loss)\n",
    "\n",
    "                \n",
    "            # Calculate gradients\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            # print(grads)\n",
    "            \n",
    "            # Apply gradients to model weights\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # # Print updated model weights\n",
    "            # for var in model.trainable_variables:\n",
    "            #     print(var.name, var.numpy()[1])\n",
    "\n",
    "\n",
    "        adjust_weights(obs_history=obs_history, action_history=action_history, discounted_rewards=discounted_rewards)\n",
    "\n",
    "\n",
    "        # Append episode rewards for plotting\n",
    "        total_reward = sum(reward_history)\n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f\"CartPole-v0 episode {episode}, reward sum: {total_reward}\")\n",
    "\n",
    "        # clear_output(wait=True)\n",
    "        \n",
    "    env.close()\n",
    "\n",
    "    # After the for loop\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\", input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(2, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "CartPole_RL(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (Pong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "GPU device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Suppress DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "def check_gpu_availability():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(\"GPU is available\")\n",
    "        for gpu in gpus:\n",
    "            print(f\"GPU device: {gpu}\")\n",
    "    else:\n",
    "        print(\"GPU is not available\")\n",
    "\n",
    "check_gpu_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pong-v0, episode 0\n",
      "Pong-v0, episode 1\n",
      "Pong-v0, episode 2\n",
      "Pong-v0, episode 3\n",
      "Pong-v0, episode 4\n",
      "Pong-v0, episode 5\n",
      "Pong-v0, episode 6\n",
      "Pong-v0, episode 7\n",
      "Pong-v0, episode 8\n",
      "Pong-v0, episode 9\n"
     ]
    }
   ],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    # In preprocess function\n",
    "    return np.reshape(image.astype(np.float64).ravel(), [80, 80])\n",
    "\n",
    "\n",
    "env = gym.make(\"Pong-v0\", \n",
    "            #    render_mode='human'\n",
    "               )\n",
    "\n",
    "# Roll out 10 episdoes\n",
    "for episode in range(10):\n",
    "    print(f\"Pong-v0, episode {episode}\")\n",
    "    # Initiate one episode\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    obs_history = []\n",
    "    reward_history = []\n",
    "    action_history = []\n",
    "\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    # Roll out one episode\n",
    "    while (not terminated) and (not truncated):\n",
    "        \n",
    "        # Policy (Action)\n",
    "        action = env.action_space.sample()\n",
    "        action_history.append(action)\n",
    "\n",
    "        # Preprocess observation\n",
    "        processed_observation = preprocess(observation)\n",
    "        obs_history.append(processed_observation)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # print(processed_observation.shape, reward, terminated, truncated, info)\n",
    "        \n",
    "        reward_history.append(reward)\n",
    "        \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 2D float array \"\"\"\n",
    "    image = image[35:195] # crop\n",
    "    image = image[::2,::2,0] # downsample by factor of 2\n",
    "    image[image == 144] = 0 # erase background (background type 1)\n",
    "    image[image == 109] = 0 # erase background (background type 2)\n",
    "    image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    # In preprocess function\n",
    "    return np.reshape(image.astype(np.float64).ravel(), [80, 80])\n",
    "\n",
    "def build_model(input_shape=(80, 80, 1), num_choices=6, reg=0.0001):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(reg), input_shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(reg)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(num_choices, activation=\"softmax\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def select_action(model, observation, num_choices=6):\n",
    "    probabilities = model.predict(np.array([observation]), verbose=0)[0]\n",
    "    # print(probabilities)\n",
    "    return np.random.choice(num_choices, p=probabilities)\n",
    "\n",
    "def compute_discounted_rewards(reward_history, discount_factor=0.99):\n",
    "    discounted_rewards, cumulative_reward = [], 0\n",
    "    for reward in reversed(reward_history):\n",
    "        cumulative_reward = reward + discount_factor * cumulative_reward\n",
    "        discounted_rewards.insert(0, cumulative_reward)\n",
    "    return discounted_rewards\n",
    "\n",
    "def adjust_weights(model, optimizer, obs_history, action_history, discounted_rewards):\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs=model(np.array(obs_history))\n",
    "        indices=tf.stack([tf.range(len(action_history),dtype=tf.int32),tf.convert_to_tensor(action_history,dtype=tf.int32)],axis=1)\n",
    "        chosen_probs=tf.gather_nd(probs,indices)\n",
    "        loss=-tf.math.log(chosen_probs)*discounted_rewards\n",
    "        loss=tf.reduce_mean(loss)\n",
    "    grads=tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "def Pong_RL():\n",
    "    env = gym.make(\"Pong-v0\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    model = build_model()\n",
    "    episode_rewards = []\n",
    "    episode = 0\n",
    "\n",
    "    consecutive_21_rewards = 0  # Count number of 21 occurences\n",
    "    should_train = True  # Initialize flag for training\n",
    "\n",
    "    while True:\n",
    "        observation, info = env.reset()\n",
    "        obs_history, reward_history, action_history = [], [], []\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            processed_observation = preprocess(observation)\n",
    "            action = select_action(model, processed_observation)\n",
    "            obs_history.append(processed_observation)\n",
    "            action_history.append(action)\n",
    "\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            reward_history.append(reward)\n",
    "\n",
    "        discounted_rewards = compute_discounted_rewards(reward_history)\n",
    "\n",
    "        total_reward = sum(reward_history)\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        moving_num, window = 21, 100\n",
    "        if episode >= window-1:\n",
    "            moving_avg = np.mean(episode_rewards[-window:])\n",
    "            print(f\"Pong-v0 episode {episode}, reward sum: {total_reward}, last {window} avg: {moving_avg:.2f}\")\n",
    "            \n",
    "            if moving_avg > moving_num:\n",
    "                print(f\"Stopping as the last {window}-episode moving average is greater than {moving_num}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Pong-v0 episode {episode}, reward sum: {total_reward}\")\n",
    "\n",
    "        ### TRAINING STOP\n",
    "\n",
    "        if total_reward == 21:  # Check for consecutive rewards of 21\n",
    "            consecutive_21_rewards += 1\n",
    "            if consecutive_21_rewards >= 10 and should_train == True:\n",
    "                print(\"Stopping training as the reward has been 21 for 10 episodes in a row\")\n",
    "                should_train = False  # Set flag to False\n",
    "\n",
    "                # Check if folder exists, if not create it\n",
    "                if not os.path.exists(\"saved_model\"):\n",
    "                    os.makedirs(\"saved_model\")\n",
    "\n",
    "                # Save the model\n",
    "                model.save(\"saved_model/pong_model\")\n",
    "                \n",
    "        else:\n",
    "            consecutive_21_rewards = 0  # Reset the counter if the reward is not 21\n",
    "\n",
    "        # Modify the weight adjustment to respect the should_train flag\n",
    "        if should_train:\n",
    "            adjust_weights(model, optimizer, obs_history, action_history, discounted_rewards)\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Save Results\n",
    "    if not os.path.exists('artifacts'):\n",
    "        os.makedirs('artifacts')\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.savefig('artifacts/pong_rewards.png')\n",
    "\n",
    "if __name__ == \"main\":\n",
    "    check_gpu_availability()\n",
    "    Pong_RL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\490_msia\\01_hw\\01_hw_code.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Pong_RL()\n",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\490_msia\\01_hw\\01_hw_code.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X12sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminated \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m truncated:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X12sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     processed_observation \u001b[39m=\u001b[39m preprocess(observation)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     action \u001b[39m=\u001b[39m select_action(model, processed_observation)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X12sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     obs_history\u001b[39m.\u001b[39mappend(processed_observation)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X12sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     action_history\u001b[39m.\u001b[39mappend(action)\n",
      "\u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\490_msia\\01_hw\\01_hw_code.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_action\u001b[39m(model, observation, num_choices\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     probabilities \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49marray([observation]), verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# print(probabilities)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nuke2/Desktop/NW%20Work/Fall_02%20Work/MSIA-FQ2/490_msia/01_hw/01_hw_code.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(num_choices, p\u001b[39m=\u001b[39mprobabilities)\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\keras\\engine\\training.py:2249\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2247\u001b[0m callbacks\u001b[39m.\u001b[39mon_predict_begin()\n\u001b[0;32m   2248\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2249\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2250\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   2251\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1307\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1305\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1307\u001b[0m     data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1308\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1309\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    498\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 499\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    693\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    694\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    695\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 696\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    698\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:721\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(ds_variant):\n\u001b[0;32m    717\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource \u001b[39m=\u001b[39m (\n\u001b[0;32m    718\u001b[0m       gen_dataset_ops\u001b[39m.\u001b[39manonymous_iterator_v3(\n\u001b[0;32m    719\u001b[0m           output_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types,\n\u001b[0;32m    720\u001b[0m           output_shapes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 721\u001b[0m   gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n",
      "File \u001b[1;32mc:\\Users\\nuke2\\Desktop\\NW Work\\Fall_02 Work\\MSIA-FQ2\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3408\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3406\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3407\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3408\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3409\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[0;32m   3410\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3411\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Pong_RL()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
